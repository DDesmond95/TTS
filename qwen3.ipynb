{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a849f28f",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "1. `pip install qwen-tts`\n",
    "2. `pip uninstall -y torch torchaudio torchvision`\n",
    "3. `conda install pytorch torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "155df9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"PYTORCH_SDP_ENABLED\"] = \"0\"\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"0\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f81783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch CUDA runtime: 12.1\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce GTX 1660 Ti\n",
      "Capability: (7, 5)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "\n",
    "print(\"Torch CUDA runtime:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"Capability:\", torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdc445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "819e232c",
   "metadata": {},
   "source": [
    "### Install SOX\n",
    "\n",
    "#### Step 1Ô∏è‚É£ Download SoX (official source)\n",
    "\n",
    "Go here (official, no tricks):\n",
    "\n",
    "üëâ [http://sox.sourceforge.net/](http://sox.sourceforge.net/)\n",
    "\n",
    "Click **‚ÄúDownload SoX for Windows‚Äù**\n",
    "Get the **Windows binary installer** (usually `sox-14.4.x-win32.exe` or similar).\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2Ô∏è‚É£ Install it (IMPORTANT options)\n",
    "\n",
    "During installation:\n",
    "\n",
    "- ‚úÖ **Check ‚ÄúAdd SoX to PATH‚Äù**\n",
    "- Leave everything else default\n",
    "- Finish install\n",
    "\n",
    "This PATH checkbox is the most important part.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3Ô∏è‚É£ Verify SoX works (outside Python)\n",
    "\n",
    "Open a **new terminal** (important), then run:\n",
    "\n",
    "```bat\n",
    "sox --version\n",
    "```\n",
    "\n",
    "Expected output (example):\n",
    "\n",
    "```\n",
    "sox: SoX v14.4.2\n",
    "```\n",
    "\n",
    "If you see that ‚Üí SoX is installed correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657b0082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********\n",
      "Warning: flash-attn is not installed. Will only run the manual PyTorch version. Please install flash-attn for faster inference.\n",
      "********\n",
      " \n",
      "===== ENVIRONMENT =====\n",
      "Device selected : cuda\n",
      "Torch version   : 2.2.2+cu121\n",
      "CUDA available  : True\n",
      "CUDA version    : 12.1\n",
      "GPU name        : Quadro P5000\n",
      "GPU memory (GB) : 15.88\n",
      "=======================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from qwen_tts import Qwen3TTSModel\n",
    "import gc\n",
    "\n",
    "# -----------------------------\n",
    "# Environment info\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "print(\"===== ENVIRONMENT =====\")\n",
    "print(\"Device selected :\", device)\n",
    "print(\"Torch version   :\", torch.__version__)\n",
    "print(\"CUDA available  :\", torch.cuda.is_available())\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(\"CUDA version    :\", torch.version.cuda)\n",
    "    print(\"GPU name        :\", torch.cuda.get_device_name(0))\n",
    "    print(\n",
    "        \"GPU memory (GB) :\",\n",
    "        round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2),\n",
    "    )\n",
    "print(\"=======================\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model loader with full prints\n",
    "# -----------------------------\n",
    "def load_model(kind: str):\n",
    "    print(f\"\\n===== LOADING MODEL ({kind.upper()}) =====\")\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        print(\"CUDA cache cleared\")\n",
    "\n",
    "    if kind == \"base\":\n",
    "        model_path = \"./models/Qwen3-TTS-12Hz-1.7B-Base\"\n",
    "    elif kind == \"custom\":\n",
    "        model_path = \"./models/Qwen3-TTS-12Hz-1.7B-CustomVoice\"\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'base' or 'custom'\")\n",
    "\n",
    "    print(\"Model path      :\", model_path)\n",
    "    print(\"Torch dtype     :\", dtype)\n",
    "\n",
    "    model = Qwen3TTSModel.from_pretrained(\n",
    "        model_path,\n",
    "        dtype=dtype,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Supported languages\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        langs = model.get_supported_languages()\n",
    "        print(\"Supported languages:\")\n",
    "        for lang in langs:\n",
    "            print(\" -\", lang)\n",
    "    except Exception as e:\n",
    "        print(\"Could not query supported languages:\", e)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Supported speakers (CustomVoice only)\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        speakers = model.get_supported_speakers()\n",
    "        if speakers:\n",
    "            print(\"Supported speakers:\")\n",
    "            for spk in speakers:\n",
    "                print(\" -\", spk)\n",
    "        else:\n",
    "            print(\"Supported speakers: None (Base model)\")\n",
    "    except Exception:\n",
    "        print(\"Supported speakers: Not available for this model\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # VRAM usage\n",
    "    # -----------------------------\n",
    "    if device == \"cuda\":\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        peak = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "\n",
    "        print(f\"VRAM allocated  : {allocated:.2f} GB\")\n",
    "        print(f\"VRAM reserved   : {reserved:.2f} GB\")\n",
    "        print(f\"VRAM peak       : {peak:.2f} GB\")\n",
    "\n",
    "    print(\"Model loaded ‚úî\")\n",
    "    print(\"=================================\\n\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model unload helper\n",
    "# -----------------------------\n",
    "def unload_model(model=None, name: str = \"\"):\n",
    "    print(f\"\\n===== UNLOADING MODEL {name.upper()} =====\")\n",
    "\n",
    "    if model is not None:\n",
    "        try:\n",
    "            del model\n",
    "            print(\"Model object deleted\")\n",
    "        except Exception as e:\n",
    "            print(\"Model delete warning:\", e)\n",
    "\n",
    "    gc.collect()\n",
    "    print(\"Python GC collected\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        print(\"CUDA cache cleared\")\n",
    "        print(\n",
    "            \"VRAM after cleanup:\",\n",
    "            f\"{torch.cuda.memory_allocated(0) / 1024**3:.2f} GB allocated\",\n",
    "        )\n",
    "\n",
    "    print(\"====================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a1d9bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LOADING MODEL (BASE) =====\n",
      "CUDA cache cleared\n",
      "Model path      : ./models/Qwen3-TTS-12Hz-1.7B-Base\n",
      "Torch dtype     : torch.float16\n",
      "Supported languages:\n",
      " - auto\n",
      " - chinese\n",
      " - english\n",
      " - french\n",
      " - german\n",
      " - italian\n",
      " - japanese\n",
      " - korean\n",
      " - portuguese\n",
      " - russian\n",
      " - spanish\n",
      "Supported speakers: None (Base model)\n",
      "VRAM allocated  : 1.37 GB\n",
      "VRAM reserved   : 1.44 GB\n",
      "VRAM peak       : 1.37 GB\n",
      "Model loaded ‚úî\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c0da126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== UNLOADING MODEL BASE =====\n",
      "Model object deleted\n",
      "Python GC collected\n",
      "CUDA cache cleared\n",
      "VRAM after cleanup: 1.37 GB allocated\n",
      "====================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unload_model(model, name=\"base\")\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92e0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c649aa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LOADING MODEL (CUSTOM) =====\n",
      "CUDA cache cleared\n",
      "Model path      : ./models/Qwen3-TTS-12Hz-1.7B-CustomVoice\n",
      "Torch dtype     : torch.float16\n",
      "Supported languages:\n",
      " - auto\n",
      " - chinese\n",
      " - english\n",
      " - french\n",
      " - german\n",
      " - italian\n",
      " - japanese\n",
      " - korean\n",
      " - portuguese\n",
      " - russian\n",
      " - spanish\n",
      "Supported speakers:\n",
      " - aiden\n",
      " - dylan\n",
      " - eric\n",
      " - ono_anna\n",
      " - ryan\n",
      " - serena\n",
      " - sohee\n",
      " - uncle_fu\n",
      " - vivian\n",
      "VRAM allocated  : 2.81 GB\n",
      "VRAM reserved   : 2.88 GB\n",
      "VRAM peak       : 2.81 GB\n",
      "Model loaded ‚úî\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CustomVoice\n",
    "model = load_model(\"custom\")\n",
    "speakers = model.get_supported_speakers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa1b5307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def run_generate_custom_voice(\n",
    "    model,\n",
    "    text,\n",
    "    speaker,\n",
    "    output_path,\n",
    "    language=\"auto\",\n",
    "    instruct=None,\n",
    "    generation_kwargs=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Explicit runner for Qwen3 CustomVoice TTS.\n",
    "\n",
    "    This function does NOT inject defaults or modify user intent.\n",
    "    All generation behavior is controlled by the caller.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Qwen3TTSModel\n",
    "        Loaded CustomVoice model.\n",
    "    text : str or list[str]\n",
    "        Text(s) to synthesize.\n",
    "    speaker : str or list[str]\n",
    "        Speaker name(s), must match model.get_supported_speakers().\n",
    "    output_path : str or Path\n",
    "        Output WAV file (single) or directory (batch).\n",
    "    language : str or list[str], default=\"auto\"\n",
    "        Language(s). Use \"auto\" for multilingual sentences.\n",
    "    instruct : str or list[str] or None\n",
    "        Optional style / emotion instruction(s).\n",
    "    generation_kwargs : dict or None\n",
    "        Extra kwargs passed directly to model.generate_custom_voice().\n",
    "        (Nothing is added automatically.)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n===== RUN generate_custom_voice =====\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Determine mode\n",
    "    # -----------------------------\n",
    "    is_batch = isinstance(text, list)\n",
    "\n",
    "    def ensure_list(value):\n",
    "        return value if isinstance(value, list) else [value]\n",
    "\n",
    "    text_list = ensure_list(text)\n",
    "    speaker_list = ensure_list(speaker)\n",
    "    language_list = ensure_list(language)\n",
    "    instruct_list = ensure_list(instruct) if instruct is not None else None\n",
    "\n",
    "    sample_count = len(text_list)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Length validation\n",
    "    # -----------------------------\n",
    "    def validate_length(values, name):\n",
    "        if values is not None and len(values) != sample_count:\n",
    "            raise ValueError(\n",
    "                f\"{name} count mismatch: got {len(values)}, expected {sample_count}\"\n",
    "            )\n",
    "\n",
    "    validate_length(speaker_list, \"Speaker\")\n",
    "    validate_length(language_list, \"Language\")\n",
    "    validate_length(instruct_list, \"Instruct\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Validate speaker & language\n",
    "    # -----------------------------\n",
    "    supported_speakers = set(model.get_supported_speakers())\n",
    "    supported_languages = set(model.get_supported_languages())\n",
    "\n",
    "    for spk in speaker_list:\n",
    "        if spk not in supported_speakers:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported speaker '{spk}'. \"\n",
    "                f\"Supported speakers: {sorted(supported_speakers)}\"\n",
    "            )\n",
    "\n",
    "    for lang in language_list:\n",
    "        if lang not in supported_languages:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported language '{lang}'. \"\n",
    "                f\"Supported languages: {sorted(supported_languages)}\"\n",
    "            )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Generation kwargs (explicit only)\n",
    "    # -----------------------------\n",
    "    if generation_kwargs is None:\n",
    "        generation_kwargs = {}\n",
    "\n",
    "    # -----------------------------\n",
    "    # Output handling\n",
    "    # -----------------------------\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    if is_batch:\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # User-facing summary\n",
    "    # -----------------------------\n",
    "    print(\"Batch mode        :\", is_batch)\n",
    "    print(\"Number of samples :\", sample_count)\n",
    "    print(\"Speaker(s)        :\", speaker_list if is_batch else speaker_list[0])\n",
    "    print(\"Language(s)       :\", language_list if is_batch else language_list[0])\n",
    "    print(\"Instruct          :\", instruct_list if instruct_list else \"None\")\n",
    "    print(\"Output path       :\", output_path.resolve())\n",
    "    print(\"Generation kwargs :\", generation_kwargs)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Generate audio\n",
    "    # -----------------------------\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        wavs, sample_rate = model.generate_custom_voice(\n",
    "            text=text_list if is_batch else text_list[0],\n",
    "            speaker=speaker_list if is_batch else speaker_list[0],\n",
    "            language=language_list if is_batch else language_list[0],\n",
    "            instruct=instruct_list if instruct_list is not None else None,\n",
    "            **generation_kwargs,\n",
    "        )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save results\n",
    "    # -----------------------------\n",
    "    if is_batch:\n",
    "        for index, waveform in enumerate(wavs):\n",
    "            output_file = output_path / f\"custom_voice_{index}.wav\"\n",
    "            sf.write(output_file, waveform, sample_rate)\n",
    "            print(\"Saved:\", output_file)\n",
    "    else:\n",
    "        sf.write(output_path, wavs[0], sample_rate)\n",
    "        print(\"Saved:\", output_path)\n",
    "\n",
    "    print(\"====================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c864a419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== RUN generate_custom_voice =====\n",
      "Batch mode        : False\n",
      "Number of samples : 1\n",
      "Speaker(s)        : ryan\n",
      "Language(s)       : auto\n",
      "Instruct          : None\n",
      "Output path       : /workspace/outputs/multilang_ryan.wav\n",
      "Generation kwargs : {}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmEx( handle, opa, opb, m, n, k, alpha_ptr, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, beta_ptr, c, std::is_same_v<C_Dtype, float> ? CUDA_R_32F : CUDA_R_16F, ldc, compute_type, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello everyone, today I want to show you something very interesting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mËøô‰∏™Ê®°ÂûãÁúüÁöÑÂæàÂéâÂÆ≥Ôºåboleh cakap English, Bahasa Melayu, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‰∏çÊòØÂàªÊÑèÁöÑÈÇ£ÁßçÔºåËÄåÊòØËá™ÁÑ∂ÊµÅÁïÖ„ÄÇ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[43mrun_generate_custom_voice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mryan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutputs/multilang_ryan.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 122\u001b[0m, in \u001b[0;36mrun_generate_custom_voice\u001b[0;34m(model, text, speaker, output_path, language, instruct, generation_kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Generate audio\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m--> 122\u001b[0m     wavs, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_custom_voice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspeaker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeaker_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mspeaker_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlanguage_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstruct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstruct_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minstruct_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_batch:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:124\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/qwen_tts/inference/qwen3_tts_model.py:829\u001b[0m, in \u001b[0;36mQwen3TTSModel.generate_custom_voice\u001b[0;34m(self, text, speaker, language, instruct, non_streaming_mode, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m         instruct_ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize_texts([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_instruct_text(ins)])[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    827\u001b[0m gen_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_generate_kwargs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 829\u001b[0m talker_codes_list, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstruct_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstruct_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeakers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeakers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnon_streaming_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_streaming_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m wavs, fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mspeech_tokenizer\u001b[38;5;241m.\u001b[39mdecode([{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_codes\u001b[39m\u001b[38;5;124m\"\u001b[39m: c} \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m talker_codes_list])\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wavs, fs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:124\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/qwen_tts/core/models/modeling_qwen3_tts.py:2196\u001b[0m, in \u001b[0;36mQwen3TTSForConditionalGeneration.generate\u001b[0;34m(self, input_ids, instruct_ids, ref_ids, voice_clone_prompt, languages, speakers, non_streaming_mode, max_new_tokens, do_sample, top_k, top_p, temperature, subtalker_dosample, subtalker_top_k, subtalker_top_p, subtalker_temperature, eos_token_id, repetition_penalty, **kwargs)\u001b[0m\n\u001b[1;32m   2192\u001b[0m     talker_input_embed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([talker_input_embed, icl_input_embed], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;66;03m#  tts_text_first_token\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     talker_input_embed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([talker_input_embed, \n\u001b[0;32m-> 2196\u001b[0m                                     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtalker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtalker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_id\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m codec_input_emebdding[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]], \n\u001b[1;32m   2197\u001b[0m                                     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m non_streaming_mode:\n\u001b[1;32m   2199\u001b[0m         talker_input_embed \u001b[38;5;241m=\u001b[39m talker_input_embed[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# ÂéªÊéâÂéüÊú¨ÊîæËøõÂéªÁöÑtext\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/qwen_tts/core/models/modeling_qwen3_tts.py:817\u001b[0m, in \u001b[0;36mQwen3TTSTalkerResizeMLP.forward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_state):\n\u001b[0;32m--> 817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_fc2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_fc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmEx( handle, opa, opb, m, n, k, alpha_ptr, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, beta_ptr, c, std::is_same_v<C_Dtype, float> ? CUDA_R_32F : CUDA_R_16F, ldc, compute_type, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello everyone, today I want to show you something very interesting. \"\n",
    "    \"Ëøô‰∏™Ê®°ÂûãÁúüÁöÑÂæàÂéâÂÆ≥Ôºåboleh cakap English, Bahasa Melayu, \"\n",
    "    \"dan ‰∏≠Êñá together without any awkward pause. \"\n",
    "    \"Kalau di Sarawak, orang akan cakap macam ni: \"\n",
    "    \"'Aok, sik ada masalah bah, kitak rilek jak.' \"\n",
    "    \"ÊÑèÊÄùÊòØËØ¥Ôºåeverything is okay, no need to worry. \"\n",
    "    \"You see ah, this kind of multilingual speech synthesis \"\n",
    "    \"memang power, boleh campur bahasa ikut situasi. \"\n",
    "    \"‰∏çÊòØÂàªÊÑèÁöÑÈÇ£ÁßçÔºåËÄåÊòØËá™ÁÑ∂ÊµÅÁïÖ„ÄÇ\"\n",
    ")\n",
    "\n",
    "\n",
    "run_generate_custom_voice(\n",
    "    model=model,\n",
    "    text=text,\n",
    "    speaker=\"ryan\",\n",
    "    language=\"auto\",\n",
    "    output_path=\"outputs/multilang_ryan.wav\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4619b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a9309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "550db3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== UNLOADING MODEL CUSTOM =====\n",
      "Model object deleted\n",
      "Python GC collected\n",
      "====================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unload CustomVoice\n",
    "unload_model(model, name=\"custom\")\n",
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fefcf34",
   "metadata": {},
   "source": [
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124c5263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********\n",
      "Warning: flash-attn is not installed. Will only run the manual PyTorch version. Please install flash-attn for faster inference.\n",
      "********\n",
      " \n",
      "===== RUNTIME CONFIG =====\n",
      "Device      : cuda\n",
      "DType       : torch.float16\n",
      "Attention   : eager\n",
      "GPU         : NVIDIA GeForce GTX 1660 Ti\n",
      "Capability  : (7, 5)\n",
      "==========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gc\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from qwen_tts import Qwen3TTSModel\n",
    "\n",
    "\n",
    "def select_runtime():\n",
    "    if not torch.cuda.is_available():\n",
    "        return {\n",
    "            \"device\": \"cpu\",\n",
    "            \"dtype\": torch.float32,\n",
    "            \"attn_impl\": \"eager\",\n",
    "        }\n",
    "\n",
    "    major, minor = torch.cuda.get_device_capability(0)\n",
    "\n",
    "    # Ampere+ (sm >= 80)\n",
    "    if major >= 8:\n",
    "        return {\n",
    "            \"device\": \"cuda\",\n",
    "            \"dtype\": torch.float16,  # bf16 optional if you prefer\n",
    "            \"attn_impl\": \"flash_attention_2\",\n",
    "        }\n",
    "\n",
    "    # Turing / Pascal\n",
    "    return {\n",
    "        \"device\": \"cuda\",\n",
    "        \"dtype\": torch.float16,\n",
    "        \"attn_impl\": \"eager\",\n",
    "    }\n",
    "\n",
    "\n",
    "runtime = select_runtime()\n",
    "\n",
    "print(\"===== RUNTIME CONFIG =====\")\n",
    "print(\"Device      :\", runtime[\"device\"])\n",
    "print(\"DType       :\", runtime[\"dtype\"])\n",
    "print(\"Attention   :\", runtime[\"attn_impl\"])\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU         :\", torch.cuda.get_device_name(0))\n",
    "    print(\"Capability  :\", torch.cuda.get_device_capability(0))\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "MODEL_PATH = \"./models/Qwen3-TTS-12Hz-1.7B-VoiceDesign\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "tts = Qwen3TTSModel.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"cuda:0\",\n",
    "    dtype=runtime[\"dtype\"],\n",
    "    attn_implementation=runtime[\"attn_impl\"],\n",
    ")\n",
    "\n",
    "# tts.model.speech_tokenizer.model.decoder.pre_transformer.has_sliding_layers  = False\n",
    "# for layer in tts.model.speech_tokenizer.model.decoder.pre_transformer.layers:\n",
    "#     layer.attention_type = \"full_attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de042f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "c:\\Users\\CodeAlpha\\miniconda3\\envs\\tts\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:96: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# -------- Single --------\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "t0 = time.time()\n",
    "\n",
    "text = (\n",
    "    \"Âì•Âì•Ôºå‰Ω†ÂõûÊù•Âï¶ÔºüI‚Äôve been waiting for you so long alreadyÔºå\"\n",
    "    \"sampai‰∫∫ÂÆ∂ÈÉΩÂºÄÂßãÊÉ≥‰Ω†‰∫Ü„ÄÇ\"\n",
    "    \"Really, you know? ÊØè‰∏ÄÂàÜÈíüÈÉΩËßâÂæóÁâπÂà´ÊÖ¢Ôºå\"\n",
    "    \"macam masa tak mahu jalan„ÄÇ\\n\\n\"\n",
    "    \"When I heard your footsteps just nowÔºå\"\n",
    "    \"ÊàëÂøÉÈáå‰∏Ä‰∏ãÂ≠êÂ∞±‰∫ÆËµ∑Êù•‰∫ÜÔºåterus rasaÂÆâÂøÉ„ÄÇ\"\n",
    "    \"You‚Äôre finally here, kan? Jangan pergi lagi lahÔºå\"\n",
    "    \"stay with me for a while„ÄÇ\\n\\n\"\n",
    "    \"Âì•Âì•ÔºåÊä±Êàë‰∏Ä‰∏ãÂ•Ω‰∏çÂ•ΩÔºüJust a little bitÔºåkejap saja„ÄÇ\"\n",
    "    \"Êàë promise ‰∏çÂêµ‰Ω†ÔºåÂè™ÊÉ≥Èù†ÁùÄ‰Ω†Ôºå\"\n",
    "    \"Âê¨‰Ω†ÂëºÂê∏ÁöÑÂ£∞Èü≥„ÄÇ\\n\\n\"\n",
    "    \"Everything feels okay now„ÄÇ\"\n",
    "    \"Êúâ‰Ω†Âú®Ôºå‰ªÄ‰πàÈÉΩ‰∏çÊÄï‰∫Ü„ÄÇ\"\n",
    ")\n",
    "\n",
    "\n",
    "wavs, sr = tts.generate_voice_design(\n",
    "    text=text,\n",
    "    language=\"auto\",\n",
    "    instruct=(\n",
    "        \"Soft, intimate and slightly playful young female voice. \"\n",
    "        \"Tone is affectionate and clingy, with gentle pitch rises. \"\n",
    "        \"Chinese lines should sound natural and warm, \"\n",
    "        \"English calm and emotional, \"\n",
    "        \"Malay casual and light, as if speaking softly to someone very close.\"\n",
    "    ),\n",
    "    max_new_tokens=2048,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "print(f\"[VoiceDesign Single] time: {t1 - t0:.3f}s\")\n",
    "\n",
    "sf.write(\"qwen3_tts_test_voice_design_single.wav\", wavs[0], sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62631268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Batch --------\n",
    "texts = [\n",
    "    # --- Sample 1: Chinese + English + Malay (clingy / cute) ---\n",
    "    (\n",
    "        \"Âì•Âì•Ôºå‰Ω†Áªà‰∫éÂõûÊù•Âï¶„ÄÇI waited for you for so long alreadyÔºå\"\n",
    "        \"sampai‰∫∫ÂÆ∂ÈÉΩÊúâÁÇπÂßîÂ±à‰∫Ü„ÄÇ\\n\\n\"\n",
    "        \"When you‚Äôre not aroundÔºå\"\n",
    "        \"Êó∂Èó¥Ëµ∞ÂæóÁâπÂà´ÊÖ¢Ôºåmacam duniaÂÅú‰∏ãÊù•‰∏ÄÊ†∑„ÄÇ\\n\\n\"\n",
    "        \"Áé∞Âú®‰Ω†Âú®ËøôÈáå‰∫ÜÔºå\"\n",
    "        \"ÊàëÂøÉÂ∞±ÂÆâ‰∫Ü„ÄÇÂì•Âì•ÔºåÊä±‰∏Ä‰∏ãÔºåÂ•Ω‰∏çÂ•ΩÔºü\"\n",
    "    ),\n",
    "    # --- Sample 2: English + Malay (soft emotional) ---\n",
    "    (\n",
    "        \"I didn‚Äôt think you would come back so soon.\\n\"\n",
    "        \"But when I saw youÔºå\"\n",
    "        \"terus rasa lega„ÄÇ\\n\\n\"\n",
    "        \"Just stay for a bitÔºåokayÔºü\"\n",
    "        \"Jangan pergi lagi„ÄÇ\"\n",
    "    ),\n",
    "    # --- Sample 3: Chinese-dominant with light English ---\n",
    "    (\n",
    "        \"‰ªäÂ§©ÁúüÁöÑÊúâÁÇπÁ¥Ø‰∫Ü„ÄÇ\\n\"\n",
    "        \"But hearing your voiceÔºå\"\n",
    "        \"Á™ÅÁÑ∂Â∞±ËßâÂæóÊ≤°ÈÇ£‰πàÈöæÂèó‰∫Ü„ÄÇ\\n\\n\"\n",
    "        \"Êúâ‰Ω†Âú®ÔºåÂ∞±Â§ü‰∫Ü„ÄÇ\"\n",
    "    ),\n",
    "]\n",
    "languages = [\"auto\", \"auto\", \"auto\"]\n",
    "instructs = [\n",
    "    # Sample 1 instruction\n",
    "    (\n",
    "        \"Very cute, clingy, youthful female voice. \"\n",
    "        \"Higher pitch with playful intonation. \"\n",
    "        \"Chinese sounds soft and sweet, \"\n",
    "        \"English emotional and gentle, \"\n",
    "        \"Malay casual and affectionate.\"\n",
    "    ),\n",
    "    # Sample 2 instruction\n",
    "    (\n",
    "        \"Soft, calm female voice. \"\n",
    "        \"Emotionally warm and reassuring. \"\n",
    "        \"Natural pacing, no exaggeration.\"\n",
    "    ),\n",
    "    # Sample 3 instruction\n",
    "    (\n",
    "        \"Gentle, comforting female voice. \"\n",
    "        \"Slightly tired but affectionate tone. \"\n",
    "        \"Warm and intimate delivery.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "t0 = time.time()\n",
    "\n",
    "wavs, sr = tts.generate_voice_design(\n",
    "    text=texts,\n",
    "    language=languages,\n",
    "    instruct=instructs,\n",
    "    max_new_tokens=2048,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "print(f\"[VoiceDesign Batch] time: {t1 - t0:.3f}s\")\n",
    "\n",
    "for i, w in enumerate(wavs):\n",
    "    sf.write(f\"qwen3_tts_test_voice_design_batch_{i}.wav\", w, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b502b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del tts\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7477a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
